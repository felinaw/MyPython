{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    # 该类为所有其他图节点类的父类\n",
    "    def __init__(self, inputs=[]):\n",
    "        #定义每个节点的输入和输出\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        #每个节点都是其输入节点的输出节点\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播函数 继承该类的其他类会覆写该函数\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播函数，继承该类的其他类会覆写该函数\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    # 输入节点，包括神经网络输入节点，权重节点，和偏差节点\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        #定义节点数值\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        #计算节点梯度\n",
    "        self.gradients = {self:0} # initialization \n",
    "        for n in self.outputs:\n",
    "            #以下计算该节点的输出节点对该节点的梯度\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    #全连接网络层的计算\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算 y = w*x + b\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        #反向传播计算\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            #以下分别计算对inputs， weights, bias的梯度\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    #定义sigmoid函数\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        #前向 即为sigmoid函数计算\n",
    "        self.x = self.inputs[0].value   # [0] input is a list\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播计算梯度\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    # 定义平均平方误差\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        #np.mean：求平均值方法\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        #反向计算相应梯度\n",
    "        #self.m 点的数量\n",
    "        #self.diff = y - y_hat\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total number of examples = 506\nEpoch: 1, Loss: 200.267\nEpoch: 101, Loss: 9.637\nEpoch: 201, Loss: 5.707\nEpoch: 301, Loss: 5.639\nEpoch: 401, Loss: 7.195\nEpoch: 501, Loss: 5.035\nEpoch: 601, Loss: 6.260\nEpoch: 701, Loss: 3.935\nEpoch: 801, Loss: 5.222\nEpoch: 901, Loss: 4.356\nEpoch: 1001, Loss: 4.247\nEpoch: 1101, Loss: 4.104\nEpoch: 1201, Loss: 4.514\nEpoch: 1301, Loss: 4.243\nEpoch: 1401, Loss: 3.773\nEpoch: 1501, Loss: 5.023\nEpoch: 1601, Loss: 4.186\nEpoch: 1701, Loss: 3.509\nEpoch: 1801, Loss: 3.722\nEpoch: 1901, Loss: 3.634\nEpoch: 2001, Loss: 3.454\nEpoch: 2101, Loss: 3.933\nEpoch: 2201, Loss: 4.415\nEpoch: 2301, Loss: 3.894\nEpoch: 2401, Loss: 3.617\nEpoch: 2501, Loss: 2.876\nEpoch: 2601, Loss: 3.541\nEpoch: 2701, Loss: 3.843\nEpoch: 2801, Loss: 3.569\nEpoch: 2901, Loss: 3.579\nEpoch: 3001, Loss: 3.701\nEpoch: 3101, Loss: 2.897\nEpoch: 3201, Loss: 3.970\nEpoch: 3301, Loss: 4.246\nEpoch: 3401, Loss: 3.854\nEpoch: 3501, Loss: 3.679\nEpoch: 3601, Loss: 3.548\nEpoch: 3701, Loss: 3.883\nEpoch: 3801, Loss: 3.525\nEpoch: 3901, Loss: 4.166\nEpoch: 4001, Loss: 3.329\nEpoch: 4101, Loss: 4.089\nEpoch: 4201, Loss: 3.596\nEpoch: 4301, Loss: 3.341\nEpoch: 4401, Loss: 3.420\nEpoch: 4501, Loss: 3.107\nEpoch: 4601, Loss: 3.679\nEpoch: 4701, Loss: 4.125\nEpoch: 4801, Loss: 3.853\nEpoch: 4901, Loss: 3.314\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "#randn函数返回一个或一组样本，具有标准正态分布。\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "#取整\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<__main__.Input at 0x288ef4fe408>,\n <__main__.Input at 0x288ef4fec48>,\n <__main__.Input at 0x288ef504e88>,\n <__main__.Input at 0x288ef4f5088>,\n <__main__.Input at 0x288ef4f3d08>,\n <__main__.Input at 0x288ef4febc8>,\n <__main__.Linear at 0x288ef4fe288>,\n <__main__.Sigmoid at 0x288ef4fec88>,\n <__main__.Linear at 0x288ef4fed08>,\n <__main__.MSE at 0x288ef4fee08>]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([  1.10780291,  -0.85206157,  -1.78933037,  -1.71055732,\n        -2.69257329,  -6.13174685,  -1.46995613,   6.71573535,\n       -10.07011949,  -2.98023414])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(outputNode,graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    return outputNode.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[19.21767523],\n       [15.64055008],\n       [23.35986956],\n       [22.40468702],\n       [16.89463121],\n       [12.13353818],\n       [26.44694319],\n       [32.96641724],\n       [35.27919897],\n       [24.25112302],\n       [16.92448228],\n       [14.05161832],\n       [21.77211819],\n       [23.27849903],\n       [21.87075499],\n       [ 9.37185018]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(l2,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x23115dbf508>]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 381.65 248.518125\" width=\"381.65pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 248.518125 \r\nL 381.65 248.518125 \r\nL 381.65 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 39.65 224.64 \r\nL 374.45 224.64 \r\nL 374.45 7.2 \r\nL 39.65 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mad4a251e9d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.868182\" xlink:href=\"#mad4a251e9d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(51.686932 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.98321\" xlink:href=\"#mad4a251e9d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(110.62071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.098237\" xlink:href=\"#mad4a251e9d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(172.735737 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.213265\" xlink:href=\"#mad4a251e9d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(234.850765 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"303.328293\" xlink:href=\"#mad4a251e9d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(296.965793 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.443321\" xlink:href=\"#mad4a251e9d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(359.080821 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m44b0003214\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m44b0003214\" y=\"217.357922\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(26.2875 221.157141)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m44b0003214\" y=\"185.489171\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(7.2 189.28839)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m44b0003214\" y=\"153.62042\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(7.2 157.419639)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m44b0003214\" y=\"121.751669\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3000 -->\r\n      <g transform=\"translate(7.2 125.550888)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m44b0003214\" y=\"89.882919\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4000 -->\r\n      <g transform=\"translate(7.2 93.682137)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m44b0003214\" y=\"58.014168\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 5000 -->\r\n      <g transform=\"translate(7.2 61.813386)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m44b0003214\" y=\"26.145417\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 6000 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 29.944636)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#pf008556a83)\" d=\"M 54.868182 17.083636 \r\nL 61.079685 210.423412 \r\nL 67.291187 212.561902 \r\nL 73.50269 212.921857 \r\nL 79.714193 213.414645 \r\nL 85.925696 212.877467 \r\nL 92.137199 213.531352 \r\nL 98.348701 213.822909 \r\nL 104.560204 214.031512 \r\nL 110.771707 213.938775 \r\nL 116.98321 214.179941 \r\nL 123.194712 214.217205 \r\nL 129.406215 214.213343 \r\nL 135.617718 214.341104 \r\nL 141.829221 214.251927 \r\nL 148.040724 214.306311 \r\nL 154.252226 213.901304 \r\nL 160.463729 213.846462 \r\nL 166.675232 213.867966 \r\nL 172.886735 213.675299 \r\nL 179.098237 214.416536 \r\nL 185.30974 214.077286 \r\nL 191.521243 214.235923 \r\nL 197.732746 213.844322 \r\nL 203.944249 214.294882 \r\nL 210.155751 214.311574 \r\nL 216.367254 213.997037 \r\nL 222.578757 213.943721 \r\nL 228.79026 214.170802 \r\nL 235.001763 214.01766 \r\nL 241.213265 213.6523 \r\nL 247.424768 213.857994 \r\nL 253.636271 214.756364 \r\nL 259.847774 214.186274 \r\nL 266.059276 214.160737 \r\nL 272.270779 213.951214 \r\nL 278.482282 214.603519 \r\nL 284.693785 214.538058 \r\nL 290.905288 214.470528 \r\nL 297.11679 214.291429 \r\nL 303.328293 214.247044 \r\nL 309.539796 214.342893 \r\nL 315.751299 214.254968 \r\nL 321.962801 214.517878 \r\nL 328.174304 214.487993 \r\nL 334.385807 214.333063 \r\nL 340.59731 213.893874 \r\nL 346.808813 214.205306 \r\nL 353.020315 214.234536 \r\nL 359.231818 213.998422 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 39.65 224.64 \r\nL 39.65 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 374.45 224.64 \r\nL 374.45 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 39.65 224.64 \r\nL 374.45 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 39.65 7.2 \r\nL 374.45 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pf008556a83\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-7.09356653],\n       [ 6.37112696],\n       [ 4.85888709],\n       [ 7.71812819],\n       [ 4.69811938],\n       [ 5.02622148],\n       [ 9.1410029 ],\n       [ 6.76214579],\n       [ 6.58493952],\n       [ 8.93537693]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n       4.980e+00])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n       4.980e+00])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "============] - 0s 41us/step - loss: 84.4590 - mse: 84.4590\nEpoch 4809/5000\n506/506 [==============================] - 0s 36us/step - loss: 84.9473 - mse: 84.9473\nEpoch 4810/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.8097 - mse: 84.8097\nEpoch 4811/5000\n506/506 [==============================] - 0s 38us/step - loss: 85.0116 - mse: 85.0116\nEpoch 4812/5000\n506/506 [==============================] - 0s 47us/step - loss: 84.6326 - mse: 84.6326\nEpoch 4813/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.6836 - mse: 84.6836\nEpoch 4814/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7850 - mse: 84.7850\nEpoch 4815/5000\n506/506 [==============================] - 0s 42us/step - loss: 85.1088 - mse: 85.1088\nEpoch 4816/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8670 - mse: 84.8670\nEpoch 4817/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.7060 - mse: 84.7060\nEpoch 4818/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9562 - mse: 84.9562\nEpoch 4819/5000\n506/506 [==============================] - 0s 43us/step - loss: 85.0424 - mse: 85.0424\nEpoch 4820/5000\n506/506 [==============================] - 0s 43us/step - loss: 85.0793 - mse: 85.0793\nEpoch 4821/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8568 - mse: 84.8568\nEpoch 4822/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9548 - mse: 84.9548\nEpoch 4823/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.2880 - mse: 84.2880\nEpoch 4824/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9412 - mse: 84.9412\nEpoch 4825/5000\n506/506 [==============================] - 0s 51us/step - loss: 84.5149 - mse: 84.5149\nEpoch 4826/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.2136 - mse: 84.2136\nEpoch 4827/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.8575 - mse: 84.8575\nEpoch 4828/5000\n506/506 [==============================] - 0s 42us/step - loss: 85.1014 - mse: 85.1014\nEpoch 4829/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.8635 - mse: 84.8635\nEpoch 4830/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.7899 - mse: 84.7899\nEpoch 4831/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.8884 - mse: 84.8884\nEpoch 4832/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9581 - mse: 84.9581\nEpoch 4833/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9603 - mse: 84.9603\nEpoch 4834/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7370 - mse: 84.7370\nEpoch 4835/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.6451 - mse: 84.6451\nEpoch 4836/5000\n506/506 [==============================] - 0s 36us/step - loss: 84.7905 - mse: 84.7905\nEpoch 4837/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7270 - mse: 84.7270\nEpoch 4838/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6873 - mse: 84.6873\nEpoch 4839/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.7004 - mse: 84.7004\nEpoch 4840/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.8762 - mse: 84.8762\nEpoch 4841/5000\n506/506 [==============================] - 0s 47us/step - loss: 84.7138 - mse: 84.7138\nEpoch 4842/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6614 - mse: 84.6614\nEpoch 4843/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.1905 - mse: 84.1905\nEpoch 4844/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9531 - mse: 84.9531\nEpoch 4845/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.6932 - mse: 84.6932\nEpoch 4846/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.5849 - mse: 84.5849\nEpoch 4847/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8675 - mse: 84.8675\nEpoch 4848/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9318 - mse: 84.9318\nEpoch 4849/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6034 - mse: 84.6034\nEpoch 4850/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.8180 - mse: 84.8180\nEpoch 4851/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6076 - mse: 84.6076\nEpoch 4852/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.6603 - mse: 84.6603\nEpoch 4853/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9231 - mse: 84.9231\nEpoch 4854/5000\n506/506 [==============================] - 0s 34us/step - loss: 84.6512 - mse: 84.6513\nEpoch 4855/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.6448 - mse: 84.6448\nEpoch 4856/5000\n506/506 [==============================] - 0s 38us/step - loss: 85.0093 - mse: 85.0093\nEpoch 4857/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.4772 - mse: 84.4772\nEpoch 4858/5000\n506/506 [==============================] - 0s 36us/step - loss: 84.7923 - mse: 84.7923\nEpoch 4859/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8690 - mse: 84.8690\nEpoch 4860/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6518 - mse: 84.6518\nEpoch 4861/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7576 - mse: 84.7576\nEpoch 4862/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.9812 - mse: 84.9812\nEpoch 4863/5000\n506/506 [==============================] - 0s 40us/step - loss: 85.0995 - mse: 85.0995\nEpoch 4864/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.6782 - mse: 84.6782\nEpoch 4865/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.5249 - mse: 84.5249\nEpoch 4866/5000\n506/506 [==============================] - 0s 40us/step - loss: 85.1168 - mse: 85.1168\nEpoch 4867/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.8779 - mse: 84.8779\nEpoch 4868/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.7577 - mse: 84.7577\nEpoch 4869/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7629 - mse: 84.7629\nEpoch 4870/5000\n506/506 [==============================] - 0s 39us/step - loss: 85.0199 - mse: 85.0199\nEpoch 4871/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6001 - mse: 84.6001\nEpoch 4872/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.4796 - mse: 84.4796\nEpoch 4873/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.8906 - mse: 84.8906\nEpoch 4874/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8143 - mse: 84.8143\nEpoch 4875/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7409 - mse: 84.7409\nEpoch 4876/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7891 - mse: 84.7891\nEpoch 4877/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7928 - mse: 84.7928\nEpoch 4878/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6154 - mse: 84.6154\nEpoch 4879/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.5135 - mse: 84.5135\nEpoch 4880/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7000 - mse: 84.7000\nEpoch 4881/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7714 - mse: 84.7714\nEpoch 4882/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.7798 - mse: 84.7798\nEpoch 4883/5000\n506/506 [==============================] - 0s 36us/step - loss: 84.7342 - mse: 84.7342\nEpoch 4884/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9096 - mse: 84.9096\nEpoch 4885/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9224 - mse: 84.9224\nEpoch 4886/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.5780 - mse: 84.5780\nEpoch 4887/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.4905 - mse: 84.4905\nEpoch 4888/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9885 - mse: 84.9885\nEpoch 4889/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6903 - mse: 84.6903\nEpoch 4890/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6631 - mse: 84.6631\nEpoch 4891/5000\n506/506 [==============================] - 0s 49us/step - loss: 84.6374 - mse: 84.6374\nEpoch 4892/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.7770 - mse: 84.7770\nEpoch 4893/5000\n506/506 [==============================] - 0s 44us/step - loss: 84.8061 - mse: 84.8061\nEpoch 4894/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.4310 - mse: 84.4310\nEpoch 4895/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6310 - mse: 84.6310\nEpoch 4896/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.9422 - mse: 84.9422\nEpoch 4897/5000\n506/506 [==============================] - 0s 39us/step - loss: 84.6992 - mse: 84.6992\nEpoch 4898/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.8164 - mse: 84.8164\nEpoch 4899/5000\n506/506 [==============================] - 0s 47us/step - loss: 84.7844 - mse: 84.7844\nEpoch 4900/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.6283 - mse: 84.6283\nEpoch 4901/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.7278 - mse: 84.7278\nEpoch 4902/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7454 - mse: 84.7454\nEpoch 4903/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.9788 - mse: 84.9788\nEpoch 4904/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.5176 - mse: 84.5176\nEpoch 4905/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.5968 - mse: 84.5968\nEpoch 4906/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9273 - mse: 84.9273\nEpoch 4907/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9753 - mse: 84.9753\nEpoch 4908/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.7954 - mse: 84.7954\nEpoch 4909/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9281 - mse: 84.9281\nEpoch 4910/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.6952 - mse: 84.6952\nEpoch 4911/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6261 - mse: 84.6261\nEpoch 4912/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.2337 - mse: 84.2337\nEpoch 4913/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9865 - mse: 84.9865\nEpoch 4914/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9865 - mse: 84.9865\nEpoch 4915/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.7993 - mse: 84.7993\nEpoch 4916/5000\n506/506 [==============================] - 0s 40us/step - loss: 85.1833 - mse: 85.1833\nEpoch 4917/5000\n506/506 [==============================] - 0s 36us/step - loss: 84.7451 - mse: 84.7451\nEpoch 4918/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.7266 - mse: 84.7266\nEpoch 4919/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7479 - mse: 84.7479\nEpoch 4920/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.8877 - mse: 84.8877\nEpoch 4921/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.6873 - mse: 84.6873\nEpoch 4922/5000\n 32/506 [>.............................] - ETA: 0s - loss: 100.2045 - mse: 100.204506/506 [==============================] - 0s 40us/step - loss: 84.8693 - mse: 84.8693\nEpoch 4923/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6128 - mse: 84.6128\nEpoch 4924/5000\n506/506 [==============================] - 0s 39us/step - loss: 84.8276 - mse: 84.8276\nEpoch 4925/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.8547 - mse: 84.8547\nEpoch 4926/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6600 - mse: 84.6600\nEpoch 4927/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.8391 - mse: 84.8391\nEpoch 4928/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.6488 - mse: 84.6488\nEpoch 4929/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9398 - mse: 84.9398\nEpoch 4930/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.7256 - mse: 84.7256\nEpoch 4931/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.8092 - mse: 84.8092\nEpoch 4932/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.5904 - mse: 84.5904\nEpoch 4933/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8288 - mse: 84.8288\nEpoch 4934/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.9716 - mse: 84.9716\nEpoch 4935/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.5774 - mse: 84.5774\nEpoch 4936/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9029 - mse: 84.9029\nEpoch 4937/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9809 - mse: 84.9809\nEpoch 4938/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6901 - mse: 84.6901\nEpoch 4939/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7040 - mse: 84.7040\nEpoch 4940/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.9038 - mse: 84.9038\nEpoch 4941/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9340 - mse: 84.9340\nEpoch 4942/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.9579 - mse: 84.9579\nEpoch 4943/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9161 - mse: 84.9161\nEpoch 4944/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7638 - mse: 84.7638\nEpoch 4945/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.6731 - mse: 84.6731\nEpoch 4946/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7118 - mse: 84.7118\nEpoch 4947/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.8985 - mse: 84.8986\nEpoch 4948/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7579 - mse: 84.7579\nEpoch 4949/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8983 - mse: 84.8983\nEpoch 4950/5000\n506/506 [==============================] - 0s 42us/step - loss: 85.0781 - mse: 85.0781\nEpoch 4951/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.8950 - mse: 84.8950\nEpoch 4952/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7406 - mse: 84.7406\nEpoch 4953/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.5818 - mse: 84.5818\nEpoch 4954/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7300 - mse: 84.7300\nEpoch 4955/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.5121 - mse: 84.5121\nEpoch 4956/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.6052 - mse: 84.6052\nEpoch 4957/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7222 - mse: 84.7222\nEpoch 4958/5000\n506/506 [==============================] - 0s 47us/step - loss: 84.6305 - mse: 84.6305\nEpoch 4959/5000\n506/506 [==============================] - 0s 67us/step - loss: 85.0967 - mse: 85.0967\nEpoch 4960/5000\n506/506 [==============================] - 0s 49us/step - loss: 84.7048 - mse: 84.7048\nEpoch 4961/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.5499 - mse: 84.5499\nEpoch 4962/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.8056 - mse: 84.8056\nEpoch 4963/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.7333 - mse: 84.7333\nEpoch 4964/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.9979 - mse: 84.9979\nEpoch 4965/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.8647 - mse: 84.8647\nEpoch 4966/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.8511 - mse: 84.8511\nEpoch 4967/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.6718 - mse: 84.6718\nEpoch 4968/5000\n506/506 [==============================] - 0s 38us/step - loss: 85.0421 - mse: 85.0421\nEpoch 4969/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.8152 - mse: 84.8152\nEpoch 4970/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.9895 - mse: 84.9895\nEpoch 4971/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.8264 - mse: 84.8264\nEpoch 4972/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.6976 - mse: 84.6976\nEpoch 4973/5000\n506/506 [==============================] - 0s 36us/step - loss: 85.1643 - mse: 85.1643\nEpoch 4974/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.8650 - mse: 84.8650\nEpoch 4975/5000\n506/506 [==============================] - 0s 43us/step - loss: 84.6141 - mse: 84.6141\nEpoch 4976/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.9339 - mse: 84.9339\nEpoch 4977/5000\n506/506 [==============================] - 0s 42us/step - loss: 85.1229 - mse: 85.1229\nEpoch 4978/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.9443 - mse: 84.9443\nEpoch 4979/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7991 - mse: 84.7991\nEpoch 4980/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.7395 - mse: 84.7395\nEpoch 4981/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7929 - mse: 84.7929\nEpoch 4982/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.6309 - mse: 84.6309\nEpoch 4983/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.6398 - mse: 84.6399\nEpoch 4984/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.9204 - mse: 84.9204\nEpoch 4985/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.8800 - mse: 84.8800\nEpoch 4986/5000\n 32/506 [>.............................] - ETA: 0s - loss: 101.1032 - mse: 101.103506/506 [==============================] - 0s 43us/step - loss: 84.8241 - mse: 84.8241\nEpoch 4987/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.7553 - mse: 84.7553\nEpoch 4988/5000\n506/506 [==============================] - 0s 42us/step - loss: 84.7645 - mse: 84.7645\nEpoch 4989/5000\n506/506 [==============================] - 0s 38us/step - loss: 85.0449 - mse: 85.0449\nEpoch 4990/5000\n506/506 [==============================] - 0s 40us/step - loss: 84.7291 - mse: 84.7291\nEpoch 4991/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.7621 - mse: 84.7621\nEpoch 4992/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.5364 - mse: 84.5364\nEpoch 4993/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.9576 - mse: 84.9576\nEpoch 4994/5000\n506/506 [==============================] - 0s 45us/step - loss: 84.6965 - mse: 84.6965\nEpoch 4995/5000\n506/506 [==============================] - 0s 34us/step - loss: 84.7248 - mse: 84.7248\nEpoch 4996/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.6705 - mse: 84.6705\nEpoch 4997/5000\n506/506 [==============================] - 0s 41us/step - loss: 84.8787 - mse: 84.8787\nEpoch 4998/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.4624 - mse: 84.4624\nEpoch 4999/5000\n506/506 [==============================] - 0s 38us/step - loss: 84.8646 - mse: 84.8646\nEpoch 5000/5000\n506/506 [==============================] - 0s 40us/step - loss: 85.2894 - mse: 85.2894\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x2311cd97fc8>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=5000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}